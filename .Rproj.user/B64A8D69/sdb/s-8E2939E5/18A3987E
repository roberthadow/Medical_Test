{
    "contents" : "---\ntitle: \"Machine Learning Techniques on a Human Activity Dataset\"\nauthor: \"Robert Hadow\"\noutput:\n  word_document:\n    reference_docx: knitr-template.docx\n    toc: no\n  html_document:\n    fig_caption: yes\n    keep_md: yes\n    number_sections: yes\n---\n<!---  output: \n  word_document:\n     reference_docx: knitr-template.docx\n  html_document:\n    fig_caption: yes\n    keep_md: yes\n    number_sections: yes\n-->\n\n# Abstract \n\nThe PML dataset described herein was generated in a series of observations of human physical activity: dumbbell lifting.  It used magnetic orientation sensors and solid-state accelerometers and gyroscopes mounted on the subject's midsection, upper arm, wrist, and on a 1.25 kg dumbbell. The object of this project is to identify from the data a method that predicts which of five activities were being conducted: a proper dumbbell lift or four improper variants.\n\nAfter a look at the data, we eliminated variables for which 80% or more of the values were NA. We conducted a principal components analysis, which showed, in our opinion, little duplication. We tried several appraches, including CART, Receiver Operating Charactersitics, and Random Forest.\n\n# Basis Data\n\nDetectors of this type can directly measure the following at a rate of forty-five observations per second:\n\n- linear acceleration in three dimensions\n- orientation in three dimensions\n- rotation in three dimensions\n\nThe analysts computed certain statistics from the original observations.  Not all measures got secondary analysis. We would dump statistics that require too much imputation.\n\n- kurtosis\n- skewness\n- mean\n- sd\n\n\n# First Look\n\nAfter we unpacked the data, we made an ocular examination of its structure and content. See code segment _preprocess 1_.\n\n```{r preprocess_1, echo = FALSE}\n\nsuppressPackageStartupMessages(library(knitr))\nknitr::opts_chunk$set(fig.width=12, fig.height=8, echo = FALSE)\n\nif(!file.exists(\"figures\")) dir.create(\"figures\")\nfigDir <- \"figures/\"\nopts_chunk$set(fig.path = figDir)\nsquelch = TRUE # FALSE if running in console; TRUE if running in knitr\n\nif(!file.exists(\"data\")) dir.create(\"data\")\ndataDir <- \"data/\"\n\nfileURL = \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\ntargetFile = \"data/pml-training.csv\"\ndownload.file(fileURL, targetFile, mode=\"wb\")\npml.train <- read.csv(targetFile, stringsAsFactors = FALSE,  na.strings=c(\" \", \"\", \"NA\"))\n\nfileURL <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\ntargetFile <- \"./data/pml-testing.csv\"\ndownload.file(fileURL, targetFile, mode=\"wb\")\npml.test <- read.csv(targetFile, stringsAsFactors = FALSE, na.strings=c(\" \", \"\", \"NA\"))\n\ncols <- length(colnames(pml.train))\nrows <- length(rownames(pml.train))\n```\n\nThe training data set includes `r rows` rows of `r cols` columns. The direct observations are provided as numerics or integers.  Calculated values are provided as character strings.  There are many missing values (blanks) and NAs. \n\n# Method\n\n(1) Convert subjects and known exercise type to factors\n(2) Convert character columns to numeric \n(3) Drop all columns with more than 50% NA values\n(4) While testing code, limit data sets to 1.5% of original size\n(5) Investigate possibility of combining principal components\n(6) Partition data 60% training, 40% testing\n(7) Use the Random Forest algorithm twenty-five times to develop a model\n(8) Test the model against the reserved testing data\n\nWhen a satisfactory result is obtained, perform steps 1-3 on the test set provided, then transform it for submission and grading.  See code segment _preprocess 2_.\n\n```{r preprocess_2}\n\nsuppressPackageStartupMessages(library(e1071))\nsuppressPackageStartupMessages(library(R.utils))\nsuppressPackageStartupMessages(library(plyr))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(data.table))\nsuppressPackageStartupMessages(library(caret))\nsuppressPackageStartupMessages(library(randomForest))\nsuppressPackageStartupMessages(library(rocc))\nsuppressPackageStartupMessages(library(rpart))\nsuppressPackageStartupMessages(library(rmarkdown))\n\n# I used parallel during development, but ran into problems with reproduceability and thrashing\nsuppressPackageStartupMessages(library(doParallel))\ncl <- makeCluster(detectCores()-2)\nregisterDoParallel(cl)\n\npmltrain <- pml.train[, -1] # Column 1 is worthless in any case\npmltest  <- pml.test[, -1]\n\nx <- pml.test # initialize x\nCode <- c(\"cart\", \"rocc\", \"rf\")\nMethod <- c(\"CART\", \"Receiver Operating\", \"Random Forest\")\nSample_Accuracy <- Est_OOS_Accuracy <- c(0.0, 0.0, 0.0)\nresults <- data.frame(Code, Method, Sample_Accuracy, Est_OOS_Accuracy)\n\n# pml_write_files takes two-column matrix and writes out solution to assignment\npml_write_files = function(x){\n  n = nrow(x)\n  for(i in 1:n){\n    filename = paste0(\"problem_id_\",x[i,1],\".txt\")\n    write.table(x[i,2],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\n  }\n}\n\n# accuracy() fit is the result of train(). data is the training or test set 15-11-19\n# y is outcome. Should be in quotes.\naccuracy <- function(fit = modfit, data = training, y = \"classe\") {\n        prediction <- predict(fit, data)\n        data[ , \"predRight\"] <- data[ , y] == prediction \n        sum(data[ , \"predRight\"])/length(data[,\"predRight\"])\n                }\n\n# Convert some columns to factor\n# Absolutely parallel treatment two sets, training and test\n# (it would make sense to build a function, but column names are different)\npmltrain[, \"user_name\"]  <- as.factor(pmltrain[, \"user_name\"])\npmltest[,  \"user_name\"]  <- as.factor(pmltest[,  \"user_name\"])\npmltrain[, \"classe\"]     <- as.factor(pmltrain[, \"classe\"])\npmltest[,  \"problem_id\"] <- as.factor(pmltest[,  \"problem_id\"])\n\n# Convert character columns to numeric \nccols <- function(x) {\n        for (i in 1:ncol(x)) {\n        if(class(x[1, i]) == \"character\") {\n                x[,i] <- suppressWarnings(as.numeric(x[,i]))\n        }}\n        x\n}\n        \npmltrain <- ccols(pmltrain)\npmltest  <- ccols(pmltest)\n\n# Establish columns to drop from train. Exactly same ones will be dropped from test.\nna_count <-sapply(pmltrain, function(y) sum(length(which(is.na(y)))))\n\nthreshhold <- .5 * nrow(pmltrain)\nselCol = na_count < threshhold\npmltrain <- pmltrain[ , selCol]\npmltest <- pmltest[ , selCol]\n\n# TEMPORARY  Make smaller dataset\nset.seed <- 104729\n# pmltrain <- sample_frac(pmltrain, .01)\n\n# end of preprocessing_2\n```\n\n## Object of Analysis  \n\nWe seek to use the PML-Training dataset to learn to identify exercises by their characteristics, then predict from a small sample what exercise was conducted.  See code segment _PCA investigation_. \n\n```{r PCA_investigation}\n\nset.seed <- 104729\n# Copy dataset without worthless x column and subject names\nselCol  <- c(\"x\")\npmltemp <- pmltrain[, -1]\n\n# determine 80% threshhold\npreProc <- preProcess(pmltrain[, !colnames(pmltrain) == \"classe\"], method =\"pca\", thresh=.8)\ntrainPC <- predict(preProc, pmltrain[, !colnames(pmltrain) == \"classe\"])\n\nPCACount = sum(grepl(\"^PC\", colnames(preProc$rotation)))\n               \n# Try the signficant PCAs\npreProc <- preProcess(pmltrain[, !colnames(pmltrain) == \"classe\"], method=\"pca\", pcaComp=PCACount)\ntrainPC <- predict(preProc, pmltrain[, !colnames(pmltrain) == \"classe\"])\n\nplot(trainPC[, \"PC1\"], trainPC[, \"PC2\"], \n    main=\"Top Two Principal Components - not indicative of exercise\",\n    col=pmltemp[, \"classe\"], cex = 1.5,\n     ylab=\"PC2\",  xlab=\"PC1\", \n    sub = \"Five clear groupings, but not predictive of exercise\")\nlegend(\"topright\", legend = c(\"Exercise 1\", \"Exercise 2\", \"Exercise 3\",\n                             \"Exercise 4\", \"Exercise 5\"),\n                     col = c(1:5), lty = 0, pch = 1)\n\nplot(trainPC[, \"PC1\"], trainPC[,\"PC2\"], \n    main=\"Top Two Principal Components - indicative of subject\",\n    col=as.integer(factor(pmltrain$user_name)), \n     ylab=\"PC2\", xlab = \"PC1\",  cex = 1.5, \n    sub = \"Six clear groupings, reflecting the subjects\") \nlegend(\"topright\", legend = c(\"Adelmo\", \"Carlitos\",\"Charles\",\"Eurico\",\"Jeremy\",\"Pedro\"),\n                     col = c(1:5), lty = 0, pch = 1)\n\n```\n\nFirst we reduced the number of variables by PCA analysis. We chose to see how many combinations of columns could explain 80% of the variability of the data set. We were able to distill 80% of the variability of the data set into `r PCACount` variables.  Even for the first two principal measures, there is little association. Insofar as the two most correlated variables from the dataset still remaining show little correlation, we abandoned our quest to reduce the number of variables further.\n\n\n# Predicting Out-of-Sample Error\n\n$True\\ Prediction\\ Error=Training\\ Error+Training\\ Optimism$  \n\n$True\\ Prediction\\ Error=Training\\ Error+f(Model\\ Complexity)$\n\nIn the assignment, we are asked to use a cross-validation technique to check our model complexity. The simplest measure of complexity is the number of predictors.  We used holdout data as a surrogate for fresh data.  We used a random sample of 60% of our data as the training set and 40% as the holdout - a test set.  \n\nWe used our reserved test set to develop an estimate of out-of-sample error.  \n\n\n# Leave-One-Out Cross Validation\n\nWe chose not to try Leave-One-Out Cross Validation because the data set is of sufficient size not to warrant it.\n\nAnother approach to cross validation would be to run a set of analyses with one subject of six left out of the mix.  This would be particularly effective if the test set included a completely new subject (which our project does not). See code segment _most significant_.\n\n```{r most_significant}\n\nset.seed <- 104729\n\n# Convert factors back to numerics just for this test\npmltemp <- pmltrain\npmltemp[, \"user_name\"] <- as.numeric(pmltemp[, \"user_name\"])\npmltemp[, \"classe\"] <- as.numeric(pmltemp[, \"classe\"])\n#str(pmltemp[, 1:57])\nlm1 <- lm(classe ~ ., data = pmltemp)\n\ntemp <- summary(lm1)$coef\ntemp <- data.frame(temp[order(-temp[,1]), c(1,4)])\nstr  <- c(\"dumbbell\", \"belt\", \"forearm\", \"arm\")\ntemp %>% mutate(device = rownames(.), obs = 0, pos = \"xxx\") -> temp\ntemp$obs <- (grepl(\"_arm\", temp$device)) * 4 + temp$obs\ntemp$obs <- (grepl(\"forearm\", temp$device)) * 3 + temp$obs\ntemp$obs <- (grepl(\"belt\", temp$device)) * 2 + temp$obs\ntemp$obs <- (grepl(\"dumbbell\", temp$device)) * 1 + temp$obs\ntemp     <- temp[!temp$obs == 0, ]\ntemp     <- temp[order(-temp$Estimate), c(4,3,1,2,5)]\ntemp$Estimate <- abs(temp$Estimate)\nfor (i in 1:nrow(temp)) temp[i,\"pos\"] <- str[temp[i,\"obs\"]]\n\ntheme_set(theme_gray(base_size = 15))\nggplot(temp, aes(y = Estimate, x=factor(pos)))+ \n  geom_violin() + geom_point() +\n  labs(title = \"Belt and Dumbbell Measures Generally Most Significant Predicting Exercises\",\n             y = \"Absolute Value of Coefficients\", x = \"Location of Measuring Devices\")\n\nset.seed <- 104729\ntemp <- createDataPartition(y = pmltrain$classe, p=.6, list = FALSE)\ntraining <- pmltrain[ temp,]\ntesting  <- pmltrain[-temp,]\nrm(temp)\n\n```\n\n\n# CART Tree Method\n\nWe used the CART Tree method, but with less than satisfactory results. See code segment _CART investigation_.\n\n```{r CART_investigation}\n\n# Replace subject variable with five binary variables to accomodate lm\nset.seed <- 104729\ntemp <- createDataPartition(y = pmltrain$classe, p=.6, list = FALSE)\ntraining <- pmltrain[ temp,]\ntesting  <- pmltrain[-temp,]\nrm(temp)\n\nsystem.time(modFit <- train(classe ~ ., data = training, method = \"rpart\"))\n\npred <- predict(modFit, testing)\nresults[Code == \"cart\", 3] <- mean(modFit$resample$Accuracy)\nresults[Code == \"cart\", 4] <- mean(modFit$resample$Kappa)\n#x <- table(pred, testing$classe)\ntable(pred, testing$classe)\n\nresults[Code == \"cart\", 3] <- accuracy(modFit, training,\"classe\")       \nresults[Code == \"cart\", 4] <- accuracy(modFit, testing,\"classe\")       \n\n```\n\n# Receiver Operating Characteristics Method\n\nWe used the ROCC method, with better results. See code segment _rocc investigation_.\n\n\n```{r rocc}\n\nset.seed <- 104729\ntemp <- createDataPartition(y = pmltrain$classe, p=.6, list = FALSE)\ntraining <- pmltrain[ temp,]\ntesting  <- pmltrain[-temp,]\nrm(temp)\n\nsystem.time(modFit <- train(classe ~ ., data = training, method = \"rocc\")) #, prox = TRUE))\n\npred <- predict(modFit, testing)\ntesting$predRight <- pred == testing$classe\n\"ROCC Prediction Model run against reserved test set\"\ntable(pred, testing$classe)\n\nresults[Code == \"rocc\", 3] <- accuracy(modFit, training,\"classe\")       \nresults[Code == \"rocc\", 4] <- accuracy(modFit, testing,\"classe\")       \n\n```\n\n# Random Forest Method\n\nWe used the Random Forest Method with good results. See code segment _forest_ \n \n```{r forest}\n\nset.seed <- 104729\ntemp <- createDataPartition(y = pmltrain$classe, p=.6, list = FALSE)\ntraining <- pmltrain[ temp,]\ntesting  <- pmltrain[-temp,]\nrm(temp)\n\nsystem.time(modFit <- train(classe ~ ., data = training, method = \"rf\", prox = TRUE))\n\n\"Random Forest Prediction Model run against training set (along the x-axis)\"\npred <- predict(modFit, training)\ntable(pred, training$classe)\n\n\"Random Forest Prediction Model run against reserved test set (along the x-axis)\"\npred <- predict(modFit, testing)\ntable(pred, testing$classe)\n\nresults[Code == \"rf\", 3] <- accuracy(modFit, training, \"classe\")       \nresults[Code == \"rf\", 4] <- accuracy(modFit, testing,  \"classe\")    \n\n```\n\n\\pagebreak\n\n# Select Prediction Solution to Problem Set\n\nWe eliminated predictors for which 80% of the data was NA.  We looked at Principal Components.  While fourteen PCA predictors accounted for 80% of the variability, they did not show apparent correlation. We therefore chose not to combine any predictors. \n\nFirst we tried a CART prediction model, which provided reasonable but not satisfactory results \n\nWe tried an Receiver Operating Characteristic (ROC) model, which provided unsatisfactory results.\n\nIn the end, a Random Forest model worked best. It is our choice for prediction. See code segment _predict solution_.\n\n \n```{r predict_solution}\n\nresults\n\n# Rely on modFit developed earlier in forest\n# Requires pmltest have undergone all transforms that pmltrain did\n\npmltest$problem_id = c(\"01\",\"02\",\"03\",\"04\",\"05\", \"06\",\"07\",\"08\",\"09\",\"10\",\n                       \"11\",\"12\",\"13\",\"14\",\"15\", \"16\",\"17\",\"18\",\"19\",\"20\")\n\n#pmltest$predRight <- pred == pmltest$problem_id\n\"Random Forest Prediction Model run against problem set\"\npred <- predict(modFit, pmltest)\ntable(pred, pmltest$problem_id)\n\nsolution = data.frame(pmltest$problem_id, pred)\nsolution\n\npml_write_files(solution)\n\n```\n\n\\pagebreak\n\n# APPENDICES\n\n## Code\n\n### preprocess_1\n\n```{r A-preprocess_1, ref.label='preprocess_1', eval = FALSE, echo = TRUE}\n\n```\n\n### preprocess_2\n\n```{r A-preprocess_2, ref.label='preprocess_2', eval = FALSE, echo = TRUE}\n\n```\n\n\n### PCA_investigation\n\n```{r A-PCA_investigation, ref.label='PCA_investigation', eval = FALSE, echo = TRUE}\n\n```\n\n\n### CART investigation\n\n```{r A-CART_investigation, ref.label='CART_investigation', eval = FALSE, echo = TRUE}\n\n```\n\n### ROCC investigation\n\n```{r A-rocc, ref.label='rocc', eval = FALSE, echo = TRUE}\n\n```\n\n\n### forest\n\n```{r A-forest, ref.label='forest', eval = FALSE, echo = TRUE}\n\n```\n\n## Attribution\n\nThis analysis relies on a data set kindly provided by:\n\nUgulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. \n\nImportant: you are free to use this dataset for any purpose. This dataset is licensed under the Creative Commons license (CC BY-SA). The CC BY-SA license means you can remix, tweak, and build upon this work even for commercial purposes, as long as you credit the authors of the original work and you license your new creations under the identical terms we are licensing to you. This license is often compared to \"copyleft\" free and open source software licenses. All new works based on this dataset will carry the same license, so any derivatives will also allow commercial use. \n\nUse _this project_ for academic purposes may be restricted by the ethics standards of your institution.  Be aware that the text and code in this project includes certain errors and signatures to allow easy identification of derivative works. \n\nRandom Forests$\\textregistered$ is a trademark of Health Care Productivity, Inc.\n\n",
    "created" : 1448278972584.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3964122009",
    "id" : "18A3987E",
    "lastKnownWriteTime" : 1448178950,
    "path" : "~/Hadow/Clients/JHU-DST/repo/PredMachLearn/PredMachLearn.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "type" : "r_markdown"
}